{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stepped classifier\n",
    "Since the classes have different demographic frequency, attempt to classify one vs all, in the order of most common to least common."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-20 09:12:33.347312: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-20 09:12:33.508839: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/cedip/.local/lib/python3.8/site-packages/cv2/../../lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:\n",
      "2022-10-20 09:12:33.508855: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-10-20 09:12:33.535881: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-10-20 09:12:34.927354: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/cedip/.local/lib/python3.8/site-packages/cv2/../../lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:\n",
      "2022-10-20 09:12:34.927743: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/cedip/.local/lib/python3.8/site-packages/cv2/../../lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:\n",
      "2022-10-20 09:12:34.927778: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "import cv2\n",
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from functools import partial\n",
    "import pickle as pk\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Input, Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import RandomFlip, RandomRotation\n",
    "from tensorflow.keras.layers import RandomContrast, RandomBrightness\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from cascid.configs.config import DATA_DIR\n",
    "\n",
    "IMAGE_SIZE = (256,256,3)\n",
    "RANDOM_STATE = 42\n",
    "METRICS = ['loss', 'acc', 'auc']\n",
    "\n",
    "EXPERIMENT_DIR = DATA_DIR / 'experiments'\n",
    "MODEL_PATH = DATA_DIR / 'dip' / 'model_resnet34_isic_noreg_aug_raw'\n",
    "MODEL_PATH.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (7265, 256, 256, 3)\n",
      "x_test shape: (1817, 256, 256, 3)\n",
      "y_train shape: (7265, 1)\n",
      "y_test shape: (1817, 1)\n"
     ]
    }
   ],
   "source": [
    "from cascid.datasets.isic import database\n",
    "\n",
    "x_train, x_test, y_train, y_test = database.get_train_test_images_raw()\n",
    "categories = set(y_train.flatten().tolist())\n",
    "\n",
    "# OHE = OneHotEncoder(sparse=False)\n",
    "# y_train=np.array(list(map(lambda x: \"Cancer\" if x in ['basal cell carcinoma', 'melanoma', 'squamous cell carcinoma'] else \"Not\", y_train))).reshape(-1,1)\n",
    "# y_test=np.array(list(map(lambda x: \"Cancer\" if x in ['basal cell carcinoma', 'melanoma', 'squamous cell carcinoma'] else \"Not\", y_test))).reshape(-1,1)\n",
    "# y_train = OHE.fit_transform(y_train)\n",
    "# y_test = OHE.transform(y_test)\n",
    "\n",
    "print(\"x_train shape: {0}\".format(x_train.shape))\n",
    "print(\"x_test shape: {0}\".format(x_test.shape))\n",
    "print(\"y_train shape: {0}\".format(y_train.shape))\n",
    "print(\"y_test shape: {0}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 5\n",
    "indexes = []\n",
    "categories_dict = dict()\n",
    "for c in categories:\n",
    "    i = 0\n",
    "    categories_dict[c] = []\n",
    "    while len(categories_dict[c]) < sample_size:\n",
    "        if y_train[i][0] == c:\n",
    "            categories_dict[c].append(y_train[i][0])\n",
    "            indexes.append(i)\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_small = x_train[indexes]\n",
    "y_train_small = y_train[indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train_small shape: (30, 256, 256, 3)\n",
      "y_train_small shape: (30, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"x_train_small shape: {0}\".format(x_train_small.shape))\n",
    "print(\"y_train_small shape: {0}\".format(y_train_small.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['squamous cell carcinoma'],\n",
       "       ['nevus'],\n",
       "       ['nevus'],\n",
       "       ['seborrheic keratosis'],\n",
       "       ['nevus'],\n",
       "       ['seborrheic keratosis'],\n",
       "       ['seborrheic keratosis'],\n",
       "       ['actinic keratosis'],\n",
       "       ['basal cell carcinoma'],\n",
       "       ['nevus']], dtype=object)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_small[np.random.randint(0, len(y_train_small)-1, 10)] # 10 samples of y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNet(amt_64, amt_128, amt_256, amt_512, augmentation = False):\n",
    "    # Aurelien Geron, Hands-On Machine Learning with Scikit-Learn, Keras & Tensorflow.\n",
    "    DefaultConv2D = partial(keras.layers.Conv2D, kernel_size=3, strides=1, padding=\"SAME\", use_bias=False )# , kernel_regularizer=keras.regularizers.l1(l1=0.001)) \n",
    "\n",
    "    class ResidualUnit(keras.layers.Layer):\n",
    "        def __init__(self, filters, strides=1, activation=\"relu\", **kwargs):\n",
    "            super().__init__(**kwargs)\n",
    "            self.activation = keras.activations.get(activation)\n",
    "            self.main_layers = [\n",
    "                DefaultConv2D(filters, strides=strides), \n",
    "                keras.layers.BatchNormalization(),\n",
    "                self.activation,\n",
    "                DefaultConv2D(filters),\n",
    "                keras.layers.BatchNormalization(),\n",
    "                #keras.layers.SpatialDropout2D(0.2)\n",
    "            ]\n",
    "            self.skip_layers = []\n",
    "            if strides > 1:\n",
    "                self.skip_layers = [\n",
    "                    DefaultConv2D(filters, kernel_size=1, strides=strides),\n",
    "                    keras.layers.BatchNormalization()\n",
    "                ]\n",
    "        def call(self, inputs):\n",
    "            Z = inputs\n",
    "            for layer in self.main_layers:\n",
    "                Z = layer(Z)\n",
    "            skip_Z = inputs\n",
    "            for layer in self.skip_layers:\n",
    "                skip_Z = layer(skip_Z)\n",
    "            return self.activation(Z + skip_Z)\n",
    "\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(Input(shape=IMAGE_SIZE))\n",
    "    model.add(keras.layers.Rescaling(scale=1./255))\n",
    "    if augmentation:\n",
    "        model.add(RandomBrightness(factor=(-0.2, 0.2), value_range=(0.0, 1.0), seed=RANDOM_STATE)) # Randomly change brightness anywhere from -30% to +30%\n",
    "        model.add(RandomContrast(factor=0.5, seed=RANDOM_STATE)) # Randomly change contrast anywhere from -30% to +30%\n",
    "        model.add(RandomFlip(mode=\"horizontal_and_vertical\", seed=RANDOM_STATE)), # Randomly flip images either horizontally, vertically or both\n",
    "        model.add(RandomRotation(factor=(-0.2, 0.2), fill_mode=\"nearest\", interpolation=\"bilinear\", seed=RANDOM_STATE)) # Randomly rotate anywhere from -30% * 2PI to +30% * 2PI, filling gaps by using 'nearest' strategy)\n",
    "    model.add(DefaultConv2D(64, kernel_size=7, strides=2))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Activation(\"relu\"))\n",
    "    model.add(keras.layers.MaxPool2D(pool_size=3, strides=2, padding=\"SAME\"))\n",
    "    prev_filters = 64\n",
    "    for filters in [64] * amt_64 + [128] * amt_128 + [256] * amt_256 + [512] * amt_512:\n",
    "        strides = 1 if filters == prev_filters else 2\n",
    "        model.add(ResidualUnit(filters, strides=strides))\n",
    "        prev_filters = filters\n",
    "    model.add(keras.layers.SpatialDropout2D(0.2))\n",
    "    model.add(keras.layers.GlobalAvgPool2D())\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dense(2, activation=\"softmax\"))\n",
    "    return model\n",
    "def dump_results(model, history, path):\n",
    "    model.save(path)\n",
    "\n",
    "    with open(path / \"history.pkl\", \"wb\") as fl:\n",
    "        pk.dump(history, fl)\n",
    "\n",
    "def load_results(path):\n",
    "    model= load_model(path)\n",
    "\n",
    "    with open(path / \"history.pkl\", \"rb\") as fl:\n",
    "        history = pk.load(fl)\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'resnet18': (2, 2, 2, 2)\n",
    "# 'resnet34': (3, 4, 6, 3)\n",
    "\n",
    "# model = ResNet(3,4,6,3, augmentation=True)\n",
    "\n",
    "class SteppingModel():\n",
    "    '''\n",
    "    For each step,\n",
    "        Predict this step's class against rest\n",
    "        if prediction is positive\n",
    "            return predicted class,\n",
    "        else\n",
    "            pass to next step model \n",
    "    '''\n",
    "    def __init__(self, steps: List[str]) -> None:\n",
    "        self.steps = steps\n",
    "        pass\n",
    "\n",
    "    def _encode(self, y_train, target: str, target_name: str = None):\n",
    "        if target_name is None:\n",
    "            target_name = target\n",
    "        s = pd.Series(y_train.flatten())\n",
    "        s = s.apply(lambda x: target_name if x == target else \"Not\")\n",
    "        return s.to_numpy().reshape(-1,1)\n",
    "\n",
    "    def _drop(self, y_train, target: str):\n",
    "        s = pd.Series(y_train.flatten())\n",
    "        s = s[s!=target]\n",
    "        return s.to_numpy().reshape(-1,1)\n",
    "\n",
    "    def fit(self, x_train, y_train):\n",
    "        '''\n",
    "        Make one model for each step, predicting against all others (except maybe previous steps?).\n",
    "        fit them using functions above to trim and adjust data.\n",
    "        Store each model in an OrderedDict (collections), so as to maintain order of steps, while still reatining information about their classification goals. \n",
    "        '''\n",
    "        pass\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
